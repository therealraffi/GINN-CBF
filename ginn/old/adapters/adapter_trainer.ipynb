{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from GINN.problem_sampler import ProblemSampler\n",
    "from train.train_utils.latent_sampler import sample_new_z\n",
    "from utils import get_stateless_net_with_partials, get_model\n",
    "from neural_clbf.controllers.simple_neural_cbf_controller import SimpleNeuralCBFController\n",
    "from neural_clbf.systems.simple3d import Simple3DRobot\n",
    "from configs.get_config import get_config_from_yml\n",
    "from models.model_utils import tensor_product_xz\n",
    "from train.train_utils.loss_optims import LossBalancer, GradNormBalancer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from tqdm import trange\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapter network to replace the final SIREN layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim=16):\n",
    "        super(AdapterMLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Output correction term\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class ConditionalSIRENWithAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional SIREN with an adapter replacing the final layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, siren_model, adapter_model):\n",
    "        super(ConditionalSIRENWithAdapter, self).__init__()\n",
    "        \n",
    "        # Remove final linear layer from SIREN\n",
    "        self.siren = nn.Sequential(*list(siren_model.network.children())[:-1])\n",
    "        self.adapter = adapter_model  # Adapter MLP replaces final layer\n",
    "        self.jacobian = None\n",
    "\n",
    "        # Freeze all but adapter\n",
    "        for param in self.siren.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, z, calc_jacobian=False):\n",
    "        xz = torch.cat([x, z], dim=-1)  # Ensure concatenation happens before passing to the model\n",
    "        features = self.siren(xz)  # Pass through modified SIREN layers\n",
    "        h_x = self.adapter(features)  # Apply adapter MLP\n",
    "\n",
    "        if calc_jacobian:\n",
    "            self.jacobian = torch.autograd.functional.jacobian(lambda x: self.forward(x, z, calc_jacobian=False), x)\n",
    "\n",
    "        return h_x\n",
    "\n",
    "class LossTimer:\n",
    "    def __init__(self):\n",
    "        self.times = defaultdict(list)  # Store loss computation times\n",
    "        self.start_times = {}  # Store start times for ongoing loss calculations\n",
    "\n",
    "    def start(self, loss_name):\n",
    "        \"\"\"Start timing for a specific loss.\"\"\"\n",
    "        self.start_times[loss_name] = time.time()\n",
    "\n",
    "    def stop(self, loss_name):\n",
    "        \"\"\"Stop timing and log duration for a specific loss.\"\"\"\n",
    "        if loss_name in self.start_times:\n",
    "            elapsed_time = time.time() - self.start_times.pop(loss_name)\n",
    "            self.times[loss_name].append(elapsed_time)\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Prints the average and all recorded times for each loss.\"\"\"\n",
    "        print(\"\\n=== Loss Timing Summary ===\")\n",
    "        for loss_name, timings in self.times.items():\n",
    "            avg_time = sum(timings) / len(timings)\n",
    "            print(f\"{loss_name}: Avg {avg_time:.6f}s | Timings: {timings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(\"config_adapter.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract values\n",
    "DATASET_DIR = config[\"paths\"][\"dataset_dir\"]\n",
    "SIREN_CONFIG_PATH = config[\"paths\"][\"siren_config_path\"]\n",
    "MODEL_PATH = config[\"paths\"][\"model_path\"]\n",
    "MODEL_SAVE_PATH = os.path.join(config[\"paths\"][\"model_save_path\"], datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "TENSORBOARD_PORT = config[\"paths\"][\"tensorboard_port\"]\n",
    "LOG_DIR = config[\"paths\"][\"tensorboard_log_dir\"]\n",
    "\n",
    "CBF_LAMBDA = config[\"training\"][\"cbf_lambda\"]\n",
    "CBF_RELAXATION_PENALTY = config[\"training\"][\"cbf_relaxation_penalty\"]\n",
    "MAX_EPOCHS = config[\"training\"][\"max_epochs\"]\n",
    "SAVE_N_EPOCHS = config[\"training\"][\"save_n_epochs\"]\n",
    "LOSS_THRESH = config[\"training\"][\"loss_thresh\"]\n",
    "MIN_LOSS_THRESH = config[\"training\"][\"min_loss_thresh\"]\n",
    "MIN_CONTROL_NORM = config[\"training\"][\"min_control_norm\"]\n",
    "LAMBDA_RECON = config[\"training\"][\"lambda_recon\"]\n",
    "LAMBDA_DESCENT = config[\"training\"][\"lambda_descent\"]\n",
    "LAMBDA_CONTROL = config[\"training\"][\"lambda_control\"]\n",
    "LOSS_BALANCER_MODEL = config[\"training\"][\"loss_balancer_model\"]\n",
    "\n",
    "CONTROLLER_PERIOD = config[\"simulation\"][\"controller_period\"]\n",
    "SIMULATION_DT = config[\"simulation\"][\"simulation_dt\"]\n",
    "\n",
    "# Ensure model save path exists\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "    os.makedirs(MODEL_SAVE_PATH)\n",
    "\n",
    "# TensorBoard setup\n",
    "log_dir = os.path.join(LOG_DIR, f\"tensorboard_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_933523/641002580.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  siren_model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard is running at: http://localhost:6006/\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|\u001b[33m██████    \u001b[0m| 6/10 [00:30<00:20,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "=== Loss Timing Summary ===\n",
      "Reconstruction Loss: Avg 0.002368s | Timings: [0.0025861263275146484, 0.0022258758544921875, 0.002318143844604492, 0.0023987293243408203, 0.0023462772369384766, 0.0023310184478759766]\n",
      "Descent Loss: Avg 2.469436s | Timings: [2.238929033279419, 3.270869493484497, 2.3278675079345703, 2.5791780948638916, 2.196256399154663, 2.2035152912139893]\n",
      "Small Control Loss: Avg 0.000223s | Timings: [0.00022935867309570312, 0.0002219676971435547, 0.00021505355834960938, 0.00022292137145996094, 0.00022602081298828125, 0.00022530555725097656]\n",
      "Loss Balancer Computation: Avg 1.267314s | Timings: [0.8722145557403564, 1.4422128200531006, 0.9493632316589355, 2.6168084144592285, 0.7698776721954346, 0.9534075260162354]\n",
      "Saving... /scratch/rhm4nj/cral/cral-ginn/ginn/all_runs/models/adapter/2025-02-24_16-39-43/model_5.pth\n",
      "Loss tensor(0.6772, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[33m██████████\u001b[0m| 10/10 [00:46<00:00,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##### Using config\n",
    "config = get_config_from_yml(SIREN_CONFIG_PATH)\n",
    "config['device'] = DEVICE\n",
    "\n",
    "siren_model = get_model(config).to(DEVICE)\n",
    "siren_model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "netp = get_stateless_net_with_partials(siren_model, use_x_and_z_arg=True)\n",
    "\n",
    "p_sampler = ProblemSampler(config)\n",
    "z = sample_new_z(config, is_init=True).to(DEVICE)\n",
    "\n",
    "adapter_model = AdapterMLP(in_dim=256, hidden_dim=16).to(DEVICE)\n",
    "model = ConditionalSIRENWithAdapter(siren_model, adapter_model).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.adapter.parameters(), lr=1e-3)\n",
    "##### Using config\n",
    "\n",
    "##### Tensor Board\n",
    "tensorboard_port = 6006\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "tensorboard_log_file = f\"tensorboard_log_{datetime_str}\"\n",
    "log_dir = os.path.join(\"all_runs/adapter\", tensorboard_log_file)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "tensorboard_process = subprocess.Popen([\"tensorboard\", \"--logdir\", log_dir, \"--port\", str(tensorboard_port)], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "tensorboard_url = f\"http://localhost:{tensorboard_port}/\"\n",
    "print(f\"TensorBoard is running at: {tensorboard_url}\")\n",
    "##### Tensor Board\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "\n",
    "tensorboard_log_file = f\"tensorboard_log_{datetime_str}\"\n",
    "log_dir = os.path.join(\"all_runs/adapter\", tensorboard_log_file)\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Initialize TensorBoard SummaryWriter with custom log file\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "########\n",
    "\n",
    "controller_period = 0.05\n",
    "simulation_dt = 0.01\n",
    "nominal_params = {}\n",
    "scenarios = [nominal_params]\n",
    "\n",
    "dynamics_model = Simple3DRobot(\n",
    "    nominal_params,\n",
    "    dt=simulation_dt,\n",
    "    controller_dt=controller_period,\n",
    "    scenarios=scenarios,\n",
    ")\n",
    "\n",
    "cbf_controller = SimpleNeuralCBFController(\n",
    "    dynamics_model,\n",
    "    scenarios,\n",
    "    model,\n",
    "    cbf_lambda=CBF_LAMBDA,\n",
    "    cbf_relaxation_penalty=CBF_RELAXATION_PENALTY,\n",
    "    z = z,\n",
    "    device = DEVICE\n",
    ")\n",
    "\n",
    "loss_timer = LossTimer()\n",
    "\n",
    "prev_lost = 1\n",
    "min_loss = 1\n",
    "best_epoch = 1\n",
    "best_model = None\n",
    "\n",
    "lambda_recon = 1.0\n",
    "lambda_descent = 1.0\n",
    "lambda_control = 1.0\n",
    "\n",
    "loss_balancer_model = 'gradnorm' # fixed\n",
    "if loss_balancer_model == 'gradnorm':\n",
    "    loss_balancer = GradNormBalancer(num_losses=3).to(DEVICE)  # 6 loss terms\n",
    "else:\n",
    "    loss_balancer = None\n",
    "\n",
    "for epoch in trange(MAX_EPOCHS, leave=True, position=0, colour=\"yellow\"):\n",
    "    # print(\"\\n=============\", str(epoch), \"=============\")\n",
    "    opt.zero_grad()\n",
    "    cbf_controller.set_V_nn(model)\n",
    "\n",
    "    # Reconstruction Loss\n",
    "    loss_timer.start(\"Reconstruction Loss\")\n",
    "    recon_inps = torch.vstack([\n",
    "        p_sampler.sample_from_interface()[0],\n",
    "        p_sampler.sample_from_domain(),\n",
    "        p_sampler.sample_from_outer()\n",
    "    ])\n",
    "    siren_ys = siren_model(*tensor_product_xz(recon_inps, z)).squeeze(1)\n",
    "    my_ys = model(*tensor_product_xz(recon_inps, z)).squeeze(1)\n",
    "    recon_loss = (siren_ys - my_ys).square().mean()\n",
    "    loss_timer.stop(\"Reconstruction Loss\")\n",
    "\n",
    "    # Descent Loss\n",
    "    loss_timer.start(\"Descent Loss\")\n",
    "    loss_descent = torch.tensor(0.0, device=DEVICE)\n",
    "    xs_start, u_refs = p_sampler.sample_for_descent()\n",
    "    losses_list, u_opt = cbf_controller.descent_loss(xs_start, u_ref=u_refs, get_us=True)\n",
    "    loss_values = torch.stack([torch.clamp(l, min=0) for _, l in losses_list if not l.isnan()], dim=0)\n",
    "    if loss_values.numel() > 0:\n",
    "        loss_descent = loss_values.mean()\n",
    "    loss_timer.stop(\"Descent Loss\")\n",
    "\n",
    "    # Small Control Loss\n",
    "    loss_timer.start(\"Small Control Loss\")\n",
    "    loss_small_control = torch.tensor(0.0, device=DEVICE)\n",
    "    u_norm = torch.norm(u_opt, p=2, dim=1)\n",
    "    loss_small_controls = torch.clamp(MIN_CONTROL_NORM - u_norm, min=0)\n",
    "    loss_small_control = loss_small_controls.mean()\n",
    "    loss_timer.stop(\"Small Control Loss\")\n",
    "\n",
    "    # Loss Balancer\n",
    "    loss_timer.start(\"Loss Balancer Computation\")\n",
    "    losses = torch.stack([recon_loss, loss_descent, loss_small_control])\n",
    "    if loss_balancer_model == 'gradnorm':\n",
    "        loss = loss_balancer(losses, model.adapter.parameters())\n",
    "        lambdas = [l.item() for l in loss_balancer.loss_weights]\n",
    "        for i, lam in enumerate(lambdas):\n",
    "            writer.add_scalar(f\"Lambda/lambda_{i}\", loss.item(), epoch)    \n",
    "    else:\n",
    "        lambdas = torch.tensor([lambda_recon, lambda_descent, lambda_control])\n",
    "        loss = (losses * lambdas).sum()\n",
    "    loss_timer.stop(\"Loss Balancer Computation\")\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    writer.add_scalar(\"Loss/Total\", loss.item(), epoch)\n",
    "    writer.add_scalar(\"Loss/Reconstruction\", recon_loss.item(), epoch)\n",
    "    writer.add_scalar(\"Loss/Descent\", loss_descent.item(), epoch)\n",
    "    writer.add_scalar(\"Loss/Small_Control\", loss_small_control.item(), epoch)\n",
    "\n",
    "    # print(\"Current loss:\", loss.item(), \"Epoch:\", epoch, \"Delta (%):\", (abs(loss - prev_lost) / loss).item() * 100)\n",
    "    if prev_lost < loss:\n",
    "        best_epoch = epoch\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    prev_lost = loss\n",
    "    if epoch % SAVE_N_EPOCHS == 0 and epoch > 1:\n",
    "        print(epoch)\n",
    "        loss_timer.print_summary()\n",
    "        savename = os.path.join(MODEL_SAVE_PATH, f\"model_{epoch}.pth\")\n",
    "        torch.save(model.state_dict(), savename)\n",
    "        print(\"Saving...\", savename)\n",
    "        print(\"Loss\", loss)\n",
    "\n",
    "savename = os.path.join(MODEL_SAVE_PATH, f\"model_best_{best_epoch}.pth\")\n",
    "torch.save(best_model, savename)\n",
    "print(\"Best epoch:\", best_epoch)\n",
    "\n",
    "# if os.path.exists(MODEL_SAVE_PATH) and not os.listdir(MODEL_SAVE_PATH):  # Check if folder exists and is empty\n",
    "#     os.rmdir(folder_path)\n",
    "#     print(f\"Deleted empty folder: {folder_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ginn_env11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
